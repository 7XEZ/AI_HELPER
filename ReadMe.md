# AI_HELPER-TOUR: Модель вопросов и ответов для туризма

Этот проект представляет собой модель искусственного интеллекта, разработанную для ответов на вопросы, связанные с туризмом. Он использует предварительно обученную модель `rugpt3small_based_on_gpt2` от Сбербанка и дообучает ее на специализированном наборе данных вопросов и ответов о туризме. Цель проекта — создать интеллектуального помощника, способного предоставлять полезную информацию о туристических направлениях, достопримечательностях и других аспектах путешествий.




## Установка

Для запуска этого проекта вам потребуется Python 3.x и несколько библиотек. Вы можете установить их, выполнив следующую команду:

```bash
pip install transformers accelerate datasets pandas scikit-learn torch
```

Также убедитесь, что у вас установлен `huggingface_hub` для аутентификации и загрузки моделей.

```bash
pip install huggingface_hub
```

Перед запуском кода вам необходимо будет аутентифицироваться на Hugging Face. Это можно сделать, вызвав функцию `login()` из `huggingface_hub` и передав ей ваш токен Hugging Face. Пример:

```python
from huggingface_hub import login
login("ВАШ_ТОКЕН_HUGGING_FACE")
```




## Использование

Основной функционал проекта реализован в Jupyter Notebook `AI_HELPER-TOUR.ipynb`. Вы можете открыть его в любой совместимой среде (например, Jupyter Lab, Google Colab или VS Code) и выполнить ячейки по порядку.

### Подготовка данных

Проект использует CSV-файл с вопросами и ответами о туризме. Убедитесь, что файл `filled_combined_tourism_qa_dataset.csv` доступен по указанному в коде пути. Если ваш файл находится в другом месте, обновите путь в ячейке, где происходит загрузка данных:

```python
combined_data = pd.read_csv("/kaggle/input/filled-qa-dataset/filled_combined_tourism_qa_dataset.csv")
```

### Запуск обучения

После загрузки данных и инициализации модели, вы можете запустить процесс обучения, выполнив соответствующую ячейку:

```python
trainer.train()
```

Обратите внимание, что обучение может занять значительное время в зависимости от аппаратного обеспечения и размера датасета.

### Тестирование модели

После обучения модель сохраняется локально. Вы можете загрузить ее и использовать для генерации ответов на новые вопросы. Пример использования функции `generate_response`:

```python
question = "Куда сходить в Париже?"
print("Вопрос:", question)
print("Ответ:", generate_response(question))
```

Эта функция принимает вопрос в виде строки и возвращает сгенерированный моделью ответ.




## Детали модели

В основе этого проекта лежит предварительно обученная модель `rugpt3small_based_on_gpt2` от Сбербанка. Это небольшая, но мощная модель, основанная на архитектуре GPT-2, адаптированная для русского языка. Она хорошо подходит для задач генерации текста и вопросов-ответов.

Модель дообучается на специализированном наборе данных, чтобы улучшить ее производительность в домене туризма. В процессе дообучения используется `DataCollatorForLanguageModeling` для динамического маскирования, что помогает модели лучше понимать контекст и генерировать более релевантные ответы.

### Параметры обучения

Обучение проводится с использованием следующих ключевых параметров:

- **`output_dir`**: `./results` (каталог для сохранения результатов обучения)
- **`evaluation_strategy`**: `epoch` (оценка производится в конце каждой эпохи)
- **`learning_rate`**: `5e-5` (скорость обучения)
- **`per_device_train_batch_size`**: `4` (размер батча для обучения на одном устройстве)
- **`per_device_eval_batch_size`**: `4` (размер батча для оценки на одном устройстве)
- **`num_train_epochs`**: `3` (количество эпох обучения)
- **`weight_decay`**: `0.01` (коэффициент затухания весов для регуляризации)
- **`save_total_limit`**: `2` (ограничение на количество сохраняемых контрольных точек модели)
- **`fp16`**: `True` (использование 16-битной точности для ускорения обучения, если поддерживается)

Эти параметры были выбраны для достижения баланса между скоростью обучения и качеством модели.




## Набор данных

Для обучения модели используется набор данных `filled_combined_tourism_qa_dataset.csv`. Этот набор данных содержит пары вопросов и ответов, специально подобранные для тематики туризма. Качество и объем этого набора данных напрямую влияют на способность модели генерировать точные и полезные ответы.

В процессе подготовки данных вопросы и ответы объединяются в единую текстовую строку с использованием специального токена `<sep>` для их разделения. Это позволяет модели обучаться на полных диалогах, улучшая ее понимание контекста и способность к генерации связных ответов.



